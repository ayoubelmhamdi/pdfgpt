Vector Search with OpenAI Embeddings: Lucene Is All You Need

### Authors
Jimmy Lin, Ronak Pradeep, Tommaso Teofili, Jasper Xian

### Affiliations
1. David R. Cheriton School of Computer Science, University of Waterloo
2. Department of Engineering, Roma Tre University

## Abstract
This paper demonstrates vector search using OpenAI embeddings and Lucene on the MS MARCO passage ranking test collection. It challenges the idea that a dedicated vector store is necessary for utilizing deep neural networks in search. The paper shows that Lucene's hierarchical navigable small-world network (HNSW) indexes are sufficient for vector search in a bi-encoder architecture. This suggests that there is no need to introduce a separate vector store into a modern AI stack for search, as existing infrastructure can handle it.

## 1 Introduction
Recent advancements in deep neural networks have led to the use of dense vectors (embeddings) for representing content in search. This paper argues against the prevailing belief that a dedicated vector store is required for managing these dense vectors. It points out that the open-source Lucene search library, which is widely used in production infrastructure, already includes capabilities for vector search. The paper also mentions the growing popularity of embedding APIs, which simplify the generation of dense vectors. To support its arguments, the paper demonstrates vector search using OpenAI embeddings on the MS MARCO passage ranking test collection.

## 2 From Architecture to Implementation
The bi-encoder architecture encodes queries and passages into dense vectors (embeddings) and uses the dot product of these embeddings to compute relevance scores. This allows search to be treated as a nearest neighbor search problem in vector space. Hierarchical navigable small-world networks (HNSW) are commonly used for indexing and searching dense vectors. The Faiss library provides an implementation of HNSW indexes. While sparse and dense retrieval models require different techniques, hybrid approaches that combine both have shown to be effective. The paper mentions the Pyserini IR toolkit, which integrates Lucene and Faiss for sparse and dense retrieval.

The current belief is that a dedicated vector store is necessary for managing both sparse and dense retrieval models in a modern AI stack.
# Vector Search with OpenAI Embeddings: Lucene Is All You Need

## Introduction
This text discusses the use of vector search with OpenAI embeddings and argues that Lucene, a widely-used search library, is sufficient for implementing vector search. The text also presents experimental results to support this claim.

## Paragraph 1
Many startups today rely on vector stores to handle operations like CRUD and nearest neighbor search. Examples of such startups include Pinecone, Weaviate, Chroma, Milvus, and Qdrant. This challenges the idea that vector search requires a separate vector store.

## Paragraph 2
The document provides a link to a detailed document about the experiments conducted using OpenAI embeddings and Lucene.

## Paragraph 3
Adding a vector store to an existing enterprise architecture can increase complexity and interaction with other components. While vector stores offer new capabilities, it is important to consider if these capabilities can be achieved through alternative means. The term "brownfield application" refers to the development of new software systems alongside existing legacy software applications/systems.

## Paragraph 4
The latest release of Lucene (version 9) includes HNSW indexing and search capabilities, making the performance difference between Lucene and dedicated vector stores the primary distinction. This suggests that Lucene alone can fulfill the requirements of vector search.

## Paragraph 5
The experimental results on the MS MARCO passage ranking test collection demonstrate that using OpenAI's ada2 embeddings with Lucene achieves comparable effectiveness to state-of-the-art methods.

## Paragraph 6
The experiments were conducted using Anserini, a Lucene-based IR toolkit that aims to bridge the gap between academic research and real-world search applications. Anserini allows researchers to easily translate their work into platforms like Elasticsearch.

## Paragraph 7
The experiments show that state-of-the-art vector search can be implemented by combining readily available components. The logical scoring model maps to the OpenAI embedding API, while the physical retrieval model is handled by Lucene. This makes vector search accessible to a wider audience.

## Paragraph 8
The experiments focused on the MS MARCO passage ranking test collection, which consists of approximately 8.8 million passages from the web. No model training was performed as the embeddings were generated using OpenAI's API endpoint.

## Paragraph 9
The experiments used the OpenAI ada2 model for generating query and passage embeddings. The model has an input limit of 8191 tokens and an output embedding size of 1536 dimensions. The passages in the corpus were truncated to 512 tokens for consistency with previous literature.

## Paragraph 10
The average token count per passage in the corpus was computed to be 75.2. The embeddings were generated efficiently by querying the API in parallel while respecting the rate limit. Error handling logic was incorporated to handle the high volume of API calls.

## Conclusion
The experiments demonstrate that Lucene is sufficient for implementing vector search with OpenAI embeddings. This approach eliminates the need for a separate vector store and simplifies the implementation process.
In this paragraph, the author discusses their experiments with the Anserini IR toolkit and Lucene for vector search using OpenAI embeddings. They mention that Lucene restricts vectors to 1024 dimensions, which was not enough for OpenAI's 1536-dimensional embeddings. They found a workaround for this limitation. They also provide experimental results in terms of effectiveness metrics. They compare the effectiveness of OpenAI's ada2 embeddings to other models. They mention that the results may vary slightly due to the non-deterministic nature of HNSW indexing. They also provide performance figures for their server setup. The author then discusses some issues and considerations related to their demonstration, including the need for official releases of Lucene with all the necessary features and the performance comparison with other alternatives. They mention that Lucene is slower in terms of indexing speed and query throughput compared to alternatives like Faiss, but they believe that Lucene has room for performance improvements and the performance gap will decrease over time. They also mention fully managed services like Vespa as potential alternatives.
Vector search capabilities have become important in text search platforms like Elasticsearch and relational databases. One tool, pgvector, allows vector similarity search in Postgres, making it a convenient option for enterprises already using Postgres. The debate is ongoing about how to implement and deploy vector search capabilities in production systems. Some argue for a separate vector store, while others propose using the Lucene ecosystem, which is already widely used in search applications. Time will tell which approach is best.

References:
- Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023. Retrieval-based Language Models and Applications.
- Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated Machine Reading Comprehension Dataset.
- Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview of the TREC 2020 Deep Learning Track.
- Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2019. Overview of the TREC 2019 Deep Learning Track.
- Josh Devins, Julie Tibshirani, and Jimmy Lin. 2022. Aligning the Research and Practice of Building Search Applications: Elasticsearch and Pyserini.
- Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2022. From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective.
- Sebastian Hofstitter, Sheng-Chieh Lin, Jneng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling.
- Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards Unsupervised Dense Information Retrieval with Contrastive Learning.
- Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs.
- Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. Evaluating Embedding APIs for Information Retrieval.
- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering.
- Jimmy Lin. 2021. A Proposed Conceptual Framework for a Representational Approach to Information Retrieval.
- Jimmy Lin. 2022. Building a Culture of Reproducibility in Academic Research.
- Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jneng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations.
- Sheng-Chieh Lin, Minghan Li, and Jimmy Lin. 2023. Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval.
- Sheng-Chieh Lin and Jimmy Lin. 2023. A Dense Representation Framework for Lexical and Semantic Matching.
- Sheng-Chieh Lin, Jneng-Hong Yang, and Jimmy Lin. 2021b. In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval.
- Xueguang Ma, Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2022a. Document Expansions and Learned Sparse Lexical Representations for MS MARCO V1 and V2.
- Xueguang Ma, Kai Sun, Ronak Pradeep, Minghan Li, and Jimmy Lin. 2022b. Another Look at DPR: Reproduction of Training and Replication of Retrieval.
- Xueguang Ma, Tommaso Teofili, and Jimmy Lin. 2023. Anserini Gets Dense Retrieval: Integration of Lucene's HNSW Indexes.
- Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.
- Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Ro
This text discusses two research papers related to vector search using OpenAI embeddings. The first paper, titled "Vector Search with OpenAI Embeddings: Lucene Is All You Need," was written by Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk in 2021. It was presented at the 9th International Conference on Learning Representations (ICLR 2021).

The second paper, titled "Anserini: Reproducible Ranking Baselines Using Lucene," was written by Peilin Yang, Hui Fang, and Jimmy Lin in 2018. It was published in the Journal of Data and Information Quality, Volume 10, Issue 4, and spans Article 16.
