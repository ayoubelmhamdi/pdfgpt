# -------- GARBAGE TEXT -------

2207.14238v1 [eess.IV] 28 Jul 2022

arXiv

Re-thinking and Re-labeling LIDC-IDRI for
Robust Pulmonary Cancer Prediction

Hanxiao Zhang!, Xiao Gu?, Minghui Zhang!, Weihao Yu!, Liang Chen®,
Zhexin Wang*, Feng Yao?, Yun Gu!+®), and Guang-Zhong Yang!

1 Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China
geron762@sjtu.edu.cn
? Imperial College London, London, UK
3 Department of Thoracic Surgery, Shanghai Chest Hospital, Shanghai Jiao Tong
University, Shanghai, China
4 Shanghai Center for Brain Science and Brain-Inspired Technology, Shanghai, China

Abstract. The LIDC-IDRI database is the most popular benchmark
for lung cancer prediction. However, with subjective assessment from
radiologists, nodules in LIDC may have entirely different malignancy
annotations from the pathological ground truth, introducing label as-
signment errors and subsequent supervision bias during training. The
LIDC database thus requires more objective labels for learning-based
cancer prediction. Based on an extra small dataset containing 180 nod-
ules diagnosed by pathological examination, we propose to re-label LIDC
data to mitigate the effect of original annotation bias verified on this ro-
bust benchmark. We demonstrate in this paper that providing new labels
by similar nodule retrieval based on metric learning would be an effec-
tive re-labeling strategy. Training on these re-labeled LIDC nodules leads
to improved model performance, which is enhanced when new labels of
uncertain nodules are added. We further infer that re-labeling LIDC is
current an expedient way for robust lung cancer prediction while build-
ing a large pathological-proven nodule database provides the long-term
solution.

Keywords: Pulmonary nodule - Cancer prediction - Metric learning -
Re-labeling.

1 Introduction

The LIDC-IDRI (Lung Image Database Consortium and Image Database Re-
source Initiative) [I] is a leading source of public datasets. Since the introduction
of LIDC, it is used extensively for lung nodule detection and cancer prediction
using learning-based methods [4J19J18)20/7124]27/15]14] .

When searching papers in PubM with the following filter: ("deep learn-
ing” OR convolutional) AND (CT OR ’computed tomography”) AND (lung OR
pulmonary) AND (nodule OR cancer OR "nodule malignancy”) AND (prediction

 

© https: //pubmed.ncbi.nlm.nih.gov/
 


2 H. Zhang et al.

OR classification), among 53 papers assessed for eligibility of nodule malignancy
classification, 40 papers used LIDC database, 5 papers used NSLT (National
Lung Screening Trial) database] [21/22[12] (no exact nodule location provided),
and 8 papers used other individual datasets. LIDC is therefore the most popular
benchmark in cancer prediction research.

A careful examination of the LIDC database, however, reveals several poten-
tial issues for cancer prediction. During the annotation of LIDC, characteristics
of nodules were assessed by multiple radiologists, where the rating of malignancy
scores (1 to 5) was based on the assumption of a 60-year-old male smoker.
Due to the lack of clinical information, these malignancy scores were subjective.
Although a subset of LIDC cases possesses patient-based pathological diagnosis
[16], its nodule-level binary labels can not be confirmed.

Since it is hard to recapture the pathological ground truth for each LIDC nod-
ule, we apply the extra SCH-LND dataset with pathological-proven labels,
which is used not only for establishing a truthful and fair evaluation benchmark
but also for transferring pathological knowledge for different clinical indications.

In this paper, we first assess the nodule prediction performances of LIDC
driven model in six scenarios and their fine-tuning effects using SCH-LND with
detailed experiments. Having identified the problems of the undecided binary
label assignment scheme on the original LIDC database and unstable transfer
learning outcomes, we seek to re-label LIDC nodule classes by interacting with
the SCH-LND. The first re-labeling strategy adopts the state-of-the-art nod-
ule classifier as an end-to-end annotator, but it has no contribution to LIDC
re-labeling. The second strategy uses metric learning to learn similarity and dis-
crimination between the nodule pairs, which is then used to elect new LIDC
labels based on the similarity ranking in a pairwise manner between the under-
labeled LIDC nodule and each nodule of SCH-LND. Experiments show that
the models trained with re-labeled LIDC data created by metric learning model
not only resolve the bias problem of the original data but also transcend the
performance of our model, especially when the new labels of the uncertain sub-
set are added. Further statistical results demonstrate that the re-labeled LIDC
data suffers class imbalance problem, which indicates us to build a larger nodule
database with pathological-proven labels.

2 Materials

LIDC-IDRI Database: According to the practice in [17], we excluded CT
scans with slice thickness larger than 3 mm and sampled nodules identified by
at least three radiologists. We only involve solid nodules in SCH-LND and LIDC
databases because giving accurate labels for solid nodules is of great challenge.
Extra Dataset: The extra dataset called SCH-LND consists of 180 solid
nodules (90 benign/90 malignant) with exact spatial coordinates and radii. Each
sample is very rare because all the nodules are confirmed and diagnosed by
immediate pathological examination via biopsy with ethical approval.

° nttps://cdas.cancer.gov/datasets/nlst/
 


Title Suppressed Due to Excessive Length 3

To regulate variant CT formats, CT slice thickness is resampled to 1mm/pixel
if it is larger than 1 mm/pixel, while the X and Y axes are fixed to 512x512
pixels. Each pixel value is unified to the HU (Hounsfield Unit) value before
nodule volume cropping.

3 Study Design

Nodule Input 3D ResNet-18 Framework Label 1 Scenario _{Score! Quantity

1
(Malignant)
mM» Q=——> Remove

        

5 60
4 181
3 438
2
1

 

 

 

 

 

 

 

LIDC-IDRI

 

 

 

 

 

 

Extra Data

Fig. 1. Illustration of the study design for nodule cancer prediction. Case 1: training
from scratch over the LIDC database after assigning nodule labels according to the
average malignancy scores in 6 scenarios. Case 2: training over extra data based on
accurate pathological-proven labels by 5-fold cross-validation. Case 3: testing or fine-
tuning LIDC models of Case 1 using extra data.

The preliminary study follows the instructions of Fig. [I] where two types of
cases (Case 1 and Case 2) conduct training and testing in each single data domain
and one type of case (Case 3) involves domain interaction (cross-domain testing
and transfer learning) between LIDC and SCH-LND. In Case 1 and Case 3, we
identify 6 different scenarios by removing uncertain average scores (Scenarios A
and B) or setting division threshold (Scenarios C, D, E, and F) to assign binary
labels for LIDC data training. Training details are described in Section

To evaluate the model performance comprehensively, we additionally intro-
duce Specificity (also called Recall,, when treating benign as positive sample)
and Precision, (Precision in benign class) [23], besides regular evaluation metrics
including Sensitivity (Recall), Precision, Accuracy, and F1 score.

Based on the visual assessment of radiologists, human-defined nodule fea-
tures can be easily extracted and classified by a commonly used model (3D
ResNet-18 [6]), whose performance can emulate the experts’ one (Fig.
1). Many studies still put investigational efforts for better results across the
LIDC board, overlooking inaccurate radiologists’ estimations and bad model ca-
pability in the real world. However, once the same model is revalidated under
the pathological-proven benchmark (Fig.|2| Case 3, Scenario A), its drawback is
objectively revealed that LIDC model decisions take up too many false-positive

   

 

   

 

 

  
 


4 H. Zhang et al.

1

03 ———

08

07

06 aoe a
_

 

 

 

 

Ww

0s
os
03

 

o1

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Train | LIDC | extra] upc | Fr UDC) FT upc | Fr uc | Fr upc | Fr uc] Fr
Test | LIDC extra Extra extra] [extra Extra) | extra Extra extra |Extra, | xtra | Extra extra | Extra
Scen| A (a/m) A: (12/45) B:(1/5) c: (1/2345) D: (12 / 345) E: (123 /45) F: (1234/5)
Case |Case1, Case2 Case3
Sensitivity ges! [0.633] [0.978|0.74a] 0.811033] __|1.000|0.744] __[o.sea|o.667| [0.810.700 _o.100|0.489!
2 Specificity 0893} 0.600, |0.233|0.622) __0.467.0522) _jo.orijose7| _[o.022\0saa) —_|o.a7a|o.se7, _|o.933|0.567
Precision 0.904) 0.613 | 0.561) 0.663 | 0.603 0.570, 0.503} 0.632 0.503| 0.594 0.608) 0.618. 0.600) 0.530)
>= Precision_b (0.872) 0.621) 0.913] 0.709) 0.712 0.588 1.000 0.689) 0.667| 0.620. 0.717| 0.654 0.509] 0.526 |
Accurate 0889] |0.617/ _[0.60s|0.683] 0.6390578| _|o.so6|o.656| (0.506|0.606, _|o.saalo.e33, _|o.si7|o.s28'
—@-Fi Score (0895, _|o.623| _|0.713|0.702/ 0.692060 __0.669|0.684 _|0.667/0.628, _—|0.695|0.656 _|0.171|0.509
Fig. 2. Performance comparisons between different Cases or Scenarios (Scen) in Fig.
fy For instance, ‘A:(12/45)’ represents ‘Scenario A’ that treats LIDC scores 1 & 2 as
benign labels and scores 4 & 5 as malignant labels. FT denotes fine-tuning using extra

 

ata by 5-fold cross-validation based on the pre-trained model in each scenario.

predictions. These two experimental outcomes raise a suspicion that whether the
visual assessment of radiologists might have a bias toward malignant class.

To resolve this suspicion, we compare the performances of 6 scenarios in Case
3. Evidence reveals that, under the testing data from SCH-LND, the number of
false-positive predictions has a declining trend when the division threshold moves
from the benign side to the malignant side, but the bias problem is still serious
when reaching Scenario E, much less of Scenario A and B. Besides, as training
on the SCH-LND dataset from scratch can hardly obtain a high capacity model
(Fig. [2] Case 2), we use transfer learning in Case 3 to get the model fine-tuned
on the basis of weights of different pre-trained LIDC models.

Observing the inter-comparison within each scenario in Case 3, transfer learn-
ing can push scattered metric values close. However, compared with Case 2, the
fine-tuning technique would bring both positive and negative transfer, depending
upon the property of the pre-trained model.

Thus, either for training from scratch or transfer learning process, the radi-
ologists’ assessment of LIDC nodule malignancy can be hard to properly use.
In addition to its inevitable assessment errors, there is a thorny problem to as-
sign LIDC labels (how to set division threshold) and removing uncertain subset
(waste of data). We thus expect to re-label the LIDC malignancy classes with
the interaction of SCH-LND, to correct the assessment bias as well as utilize the
uncertain nodules (average score=3). Two independent approaches are described
in the following section.

 

 

4 Methods

We put forward two re-labeling strategies to obtain new ground truth labels
on the LIDC database. The first strategy generates the malignancy label from
 


Title Suppressed Due to Excessive Length 5

a machine annotator: the state-of-the-art nodule classifier that has been pre-
trained on LIDC data and fine-tuned on SCH-LND to predict nodule class. The
second strategy ranks the top nodules’ labels using a machine comparator: a
metric-based Network that measures the correlation between nodule pairs.

Considering that the knowledge from radiologists’ assessments could be a
useful resource, in each strategy, two modes of LIDC re-labeling are proposed.
For Mode 1 (Substitute): LIDC completely accepts the re-label outcomes
from other label machines. For Mode 2 (Consensus): The final LIDC re-
label results would be decided by the consensus of label machine outcomes and
its original label (Scenario A). In other words, this mode will leave behind the
nodules with the same label and discard controversial ones, which may cause
data reduction. We evaluate the LIDC re-labeling effect by using SCH-LND to
test the model which is trained with re-labeled data from scratch.

4.1 Label Induction Using Machine Annotator

The optimized model with fine-tuning technique can correct the learning bias
initiated by LIDC data. Some fine-tuned models even surpass the LIDC model
performance in large scales of evaluation metrics. We wonder whether the cur-
rent best performance model can help classify and annotate new LIDC labels.
Experiments will be conducted using two annotation models from Case 2 and
Case 3 (Scenario A) in Sectio

 

 

4.2 Similar Nodule Retrieval Using Metric Learning

Inputs Model backbone Feature vectors Similarity

Pair sampling Siamese Network
g__x2 7,
v— oe ‘same label
—
Different label
\,

 

 

 

 

2
&
<
s
&

 

\ Analogous nodule retrieval
A__ “| \
| none >= ----- > || | -->

Fig. 3. The second strategy of LIDC re-labeling that using a metric learning model to
search for the most similar nodules and give new labels.

 

=: Malignant

 

 

 

 

mean~label
of

i

Corresponding label

 

 

 

 

 

 

Similarity score

 

 

 

 

 

 

 

Metric learning provides a few-shot learning approach that aims to learn
useful representations through distance comparisons. We use Siamese Network
1) [3] in this study which consists of two networks whose parameters are tied to
each other. Parameter tying guarantees that two similar nodules will be mapped
by their respective networks to adjacent locations in feature space.

 

 

 
 


6 H. Zhang et al.

 

For training a Siamese Network in Fig. BB} we pass the inputs in the set of
pairs. Each pair is randomly chosen from SCH-LND and given the label whether
two nodules of this pair are in the same class. Then these two nodule volumes are
passed through the 3D ResNet-18 to generate a fixed-length feature vector indi-
vidually. A reasonable hypothesis is given that: if the two nodules belong to the
same class, their feature vectors should have a small distance metric; otherwise,
their feature vectors will have a large distance metric. In order to distinguish
between the same and different pairs of nodules when training, we apply con-
trastive loss over the Euclidean distance metric (similarity score) induced by the
malignancy representation.

During re-labeling, we first pair each nodule from SCH-LND used in training
up with an under-labeled LIDC nodule and sort each under-labeled nodule part-
ner by their similarity scores. Then the new LIDC label is awarded by averaging
the labels of the top 20% partner nodules in the ranking list of similarity scores.

5 Experiments and Results

5.1 Implementation

We apply 3D ResNet-18 [6] in this paper with adaptive average pooling (output
size of 1x1x1) following the final convolution layer. For the general cancer pre-
diction model, we use a fully connected layer and a Sigmoid function to output
the prediction score (binary cross-entropy loss). While for Siamese Network, we
use a fully connected layer to generate the feature vector (8 neurons). Due to
various nodule sizes, the batch size is set to 1, and group normalization [25] is
adopted after each convolution layer.

All the experiments are implemented in PyTorch with a single NVIDIA
GeForce GTX 1080 Ti GPU and learned using the Adam optimizer with
the learning rate of le-3 (100 epochs) and that of le-4 for fine-tuning in transfer
learning (50 epochs). The validation set occupies 20% of the training set in each
experiment. All the experiments and results involving or having involved the
training of SCH-LND are strictly conducted by 5-fold cross-validation.

5.2 Quantitative Evaluation

To evaluate the first strategy using machine annotator, we first use Case 2 model
to re-label LIDC nodules (a form of 5-fold cross-validation) other than the un-
certain subset (original average score = 3). The re-labeled nodules are then fed
into the 3D ResNet-18 model, which will be trained from scratch and tested on
the corresponding subset of SCH-LND for evaluation. The result (4"” row) shows
that although this action greatly fixes label bias to a balanced state, this group
of new labels can hardly build a model tested well on SCH-LND. Contrary to
common sense, the state-of-the-art nodule classifier makes re-label performance
worse (5 row), which is much lower than that of learning from scratch using
SCH-LND (2"4 row), indicating that the best model optimized with fine-tuning
 


Title Suppressed Due to Excessive Length 7

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Table 1. Performances of different re-labeling methods based on each mode of re-
labeling strategies. Under-labeled LIDC data are chosen by their original average score.
Row Method [Training] Testing [Sensitivity Precision] Precision, [Accuracy] FI
T] pasetines Case [LIDC | Extra [0.9778 0.5605 | 0.9130 | 0.6056 [0.7126
2] vaseunes | Case 2 | Extra [Extra | 0.6333 0.6129 | 0.6207 | 0.6167 | 0.6230
3 Siamese | Extra | Extra | 0.6667 | 0.6000 | 0.6250 | 0.6429 | 0.6333 | 0.6452
LIDC re-labeling
Strategy | Mode [Method [Under-label] Sensitivity [Specificity Precision [Precision, [Accuracy] FI
q Substitutel O8S€2 0.5778 | 0.5667 | 0.5714 | 0.5730 | 0.5722 [0.5746
5] anmotator La [C8 FA] y.y.4.5 | 09-4630 | 0.6667 | 0.5814 | 0.5538 | 0.5648 | 0.5155
Fen TCononansl ose2 | | 0.878 | 0.3667 | 0.5809 | 0.7500 | 0.6222 | 0.6991
7 “onsensus/Gase 3-A 0.8556 | 0.3778 | 0.5789 | 0.7234 | 0.6167 | 0.6906
8 Substitut 0.6111 | 0.6556 | 0.6395 | 0.6277 | 0.6333 [0.6250
7 Icomparatorl | Siamese 0.6778 | 0.6667 | 0.6703 | 0.6742 | 0.6722 |0.6740
TO eee Consensual 0.8000 | 0.3778 | 0.5625 | 0.6538 | 0.5889 | 0.6606
il “onsensus 0.7333 | 0.5889 | 0.6408 | 0.6883 | 0.6611 | 0.6839
technique is not suitable for LIDC re-labeling. The initial two experiments adopt-
ing Mode 2 (Consensus) achieved better comprehensive outcomes than Mode 1
(Substitute) but with low Specificity.

 

Metric learning takes a different re-label strategy that retrieves similar nod-
ules according to the distance metric. Metric learning on a small dataset can
obtain a better performance (3’ row) compared with general learning from
scratch (24 row). The re-label outcomes (8‘" and 9” row) also show great
comprehensive improvement over baselines by Mode 1, where the re-labeling of
uncertain nodules (average score=3) is an important contributing factor.

Overall, there is a trade-off between Mode 1 and Mode 2. But Mode 2 seems to
remain the LIDC bias property because testing results often have low Specificity
and introduce data reduction. Re-labeling by consensus (Mode 2) may integrate
the defects of both original labels and models, especially for malignant labels,
while re-labeling uncertain nodules can help mitigate the defect of Mode 2.

We finally re-labeled the LIDC database with the Siamese Network trained
using all of SCH-LND. As shown in Fig. [4] our re-labeled results are in broad
agreement with the low malignancy score ones. In score 3 (uncertain data), the
majority of the nodules are re-labeled to benign class, which explains the better
performance when the nodules of score 3 are assigned to benign label in Scenario
E (Fig. [2] Case 3). The new labels correct more than half of the original nodule
labels with score 4 which could be the main reason leading to the data bias.

 

 

 

 

5.3 Discussion

Re-labeling through metric learning is distinct from the general supervised model
in two notable ways. First, the input pairs generated by random sampling for
metric learning provide a data augmentation effect to overcome overfitting with
limited data. Second, under-labeled LIDC data take the average labels of top-
ranked similarity nodules to increase the confidence of label propagation. These
two points may explain why general supervised models (including fine-tuning
models) perform worse than metric learning in re-labeling task. Unfortunately,
 


8 H. Zhang et al.

 

 

 

 

 

15 Benign
0 lm Malignant
30
a
¢ 250
3 Original score=1 Original score=2 Original score=4 Original score=5
S 200 Average label=0.72 Average label=0.70 Average label=0.17 Average label=0.23
2 New label=1 New label=1 New label=0 New label=0
2 iso 152
100 85 92 89
56
50
23.
° 3 3 4
score 1 score 2 score 3 score 4 score 5
Original Average Malignancy Scores
Fig. 4. Statistical result of LIDC re-labeling nodules (benign or malignant) in terms
of original average malignancy scores, where the smooth curve describes the simplified

frequency distribution histogram of average label outputs. For each average score of 1,
2, 4, and 5, one nodule re-labeling example with the opposite class (treat score 1 and
2 as benign; treat 4 and 5 as malignant) is provided.

after re-labeling, the class imbalance problem emerged (748 versus 174), while
bringing up new limits in model training performance in the aforementioned
experiments.

Moreover, due to the lack of pathological ground truth, the relabel outcomes
of this study should always remain suspect until the LIDC clinical information
is available. Considering a number of subsequent issues that LIDC may arise,
sufficient evidence in this paper explores the motive for us to promote the ongoing
collection work of a large pathological-proven nodule database, which is expected
to become a powerful open-source database for the international medical imaging
and clinical research community.

6 Conclusion and Future Work

The LIDC-IDRI database is currently the most popular public database of lung
nodules with specific spatial coordinates and experts’ annotations. However, be-
cause of the absence of clinical information, deep learning models trained based
on this database have poor generalization capability in lung cancer prediction
and downstream tasks. To challenge the low confidence labels of LIDC, an extra
nodule dataset with pathological-proven labels was used to identify the annota-
tion bias problems of LIDC and its label assignment difficulties. With the robust
supervision of SCH-LND, we used a metric learning-based approach to re-label
LIDC data according to the similar nodule retrieval. The empirical results show
that with re-labeled LIDC data, improved performance is achieved along with
the maximization of LIDC data utilization and the subsequent class imbalance
 


Title Suppressed Due to Excessive Length 9

problem. These conclusions provide a guideline for further collection of a large
pathological-proven nodule database, which is beneficial to the community.

References

10.

11.

12.

13.

14.

15.

. Armato III, $.G., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer, C.R.,

Reeves, A.P., Zhao, B., Aberle, D.R., Henschke, C.I., Hoffman, E.A., et al.: The
lung image database consortium (lidc) and image database resource initiative (idri):
a completed reference database of lung nodules on ct scans. Medical physics 38(2),
915-931 (2011)

Bellet, A., Habrard, A., Sebban, M.: Metric learning. Synthesis Lectures on Arti-
ficial Intelligence and Machine Learning 9(1), 1-151 (2015)

Guo, Q., Feng, W., Zhou, C., Huang, R., Wan, L., Wang, S.: Learning dynamic
siamese network for visual object tracking. In: Proceedings of the IEEE interna-
tional conference on computer vision. pp. 1763-1771 (2017)

Han, F., Wang, H., Zhang, G., Han, H., Song, B., Li, L., Moore, W., Lu, H., Zhao,
H., Liang, Z.: Texture feature analysis for computer-aided diagnosis on pulmonary
nodules. Journal of digital imaging 28(1), 99-115 (2015)

Han, F., Zhang, G., Wang, H., Song, B., Lu, H., Zhao, D., Zhao, H., Liang, Z.: A
texture feature analysis for diagnosis of pulmonary nodules using lidc-idri database.
In: 2013 IEEE International Conference on Medical Imaging Physics and Engineer-
ing. pp. 14-18. IEEE (2013)

. He, K., Zhang, X., Ren, $., Sun, J.: Deep residual learning for image recognition. In:

Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770-778 (2016)

Hussein, S., Cao, K., Song, Q., Bagci, U.: Risk stratification of lung nodules us-
ing 3d cnn-based multi-task learning. In: International conference on information
processing in medical imaging. pp. 249-260. Springer (2017)

Hussein, S., Gillies, R., Cao, K., Song, Q., Bagci, U.: Tumornet: Lung nodule char-
acterization using multi-view convolutional neural network with gaussian process.
In: 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017).
pp. 1007-1010. IEEE (2017)

Kaya, M., Bilge, H.§.: Deep metric learning: A survey. Symmetry 11(9), 1066
(2019)

Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)

Koch, G., Zemel, R., Salakhutdinov, R.: Siamese neural networks for one-shot
image recognition. In: ICML deep learning workshop. vol. 2. Lille (2015)

Kramer, B.S., Berg, C.D., Aberle, D.R., Prorok, P.C.: Lung cancer screening with
low-dose helical ct: results from the national lung screening trial (nlst) (2011)
Kumar, D., Chung, A.G., Shaifee, M.J., Khalvati, F., Haider, M.A., Wong, A.:
Discovery radiomics for pathologically-proven computed tomography lung cancer
prediction. In: International Conference Image Analysis and Recognition. pp. 54—
62. Springer (2017)

Liao, Z., Xie, Y., Hu, S., Xia, Y.: Learning from ambiguous labels for lung nodule
malignancy prediction. arXiv preprint arXiv:2104.11436 (2021)

Liu, L., Dou, Q., Chen, H., Qin, J., Heng, P.A.: Multi-task deep model with margin
ranking loss for lung nodule analysis. IEEE transactions on medical imaging 39(3),
718-728 (2019)
 


10

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

26.

27.

28.

29.

H. Zhang et al.

McNitt-Gray, M.F., Armato III, $.G., Meyer, C.R., Reeves, A.P., McLennan, G.,
Pais, R.C., Freymann, J., Brown, M.S., Engelmann, R.M., Bland, P.H., et al.: The
lung image database consortium (lidc) data collection process for nodule detection
and annotation. Academic radiology 14(12), 1464-1474 (2007)

Setio, A.A.A., Traverso, A., De Bel, T., Berens, M.S., van den Bogaard, C., Cerello,
P., Chen, H., Dou, Q., Fantacci, M.E., Geurts, B., et al.: Validation, comparison,
and combination of algorithms for automatic detection of pulmonary nodules in
computed tomography images: the lunal6 challenge. Medical image analysis 42,
1-13 (2017)

Shen, W., Zhou, M., Yang, F., Dong, D., Yang, C., Zang, Y., Tian, J.: Learn-
ing from experts: Developing transferable deep features for patient-level lung can-
cer prediction. In: International Conference on Medical Image Computing and
Computer-Assisted Intervention. pp. 124—131. Springer (2016)

Shen, W., Zhou, M., Yang, F., Yang, C., Tian, J.: Multi-scale convolutional neural
networks for lung nodule classification. In: International Conference on Information
Processing in Medical Imaging. pp. 588-599. Springer (2015)

Shen, W., Zhou, M., Yang, F., Yu, D., Dong, D., Yang, C., Zang, Y., Tian, J.:
Multi-crop convolutional neural networks for lung nodule malignancy suspicious-
ness classification. Pattern Recognition 61, 663-673 (2017)

Team, N.L.S.T.R.: The national lung screening trial: overview and study design.
Radiology 258(1), 243-253 (2011)

Team, N.L.S.T.R.: Reduced lung-cancer mortality with low-dose computed tomo-
graphic screening. New England Journal of Medicine 365(5), 395-409 (2011)

Wu, B., Sun, X., Hu, L., Wang, Y.: Learning with unsure data for medical im-
age diagnosis. In: Proceedings of the IEEE International Conference on Computer
Vision. pp. 10590-10599 (2019)

Wu, B., Zhou, Z., Wang, J., Wang, Y.: Joint learning for pulmonary nodule seg-
mentation, attributes and malignancy prediction. In: 2018 IEEE 15th International
Symposium on Biomedical Imaging (ISBI 2018). pp. 1109-1113. IEEE (2018)
Wu, Y., He, K.: Group normalization. In: Proceedings of the European Conference
on Computer Vision (ECCV). pp. 3-19 (2018)

Xie, Y., Xia, Y., Zhang, J., Feng, D.D., Fulham, M., Cai, W.: Transferable multi-
model ensemble for benign-malignant lung nodule classification on chest ct. In:
International Conference on Medical Image Computing and Computer-Assisted
Intervention. pp. 656-664. Springer (2017)

Xie, Y., Xia, Y., Zhang, J., Song, Y., Feng, D., Fulham, M., Cai, W.: Knowledge-
based collaborative deep learning for benign-malignant lung nodule classification
on chest ct. IEEE transactions on medical imaging 38(4), 991-1004 (2018)
Zhang, H., Gu, Y., Qin, Y., Yao, F., Yang, G.Z.: Learning with sure data for
nodule-level lung cancer prediction. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention. pp. 570-578. Springer (2020)
Zhou, Z., Sodha, V., Siddiquee, M.M.R., Feng, R., Tajbakhsh, N., Gotway, M.B.,
Liang, J.: Models genesis: Generic autodidactic models for 3d medical image anal-
ysis. In: International Conference on Medical Image Computing and Computer-
Assisted Intervention. pp. 384-393. Springer (2019)
 



# -------- PARAPHRASSING TEXT -------

# Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction

**Authors:** Hanxiao Zhang, Xiao Gu, Minghui Zhang, Weihao Yu, Liang Chen, Zhexin Wang, Feng Yao, Yun Gu, and Guang-Zhong Yang

**Affiliations:** 
1. Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China
2. Imperial College London, London, UK
3. Department of Thoracic Surgery, Shanghai Chest Hospital, Shanghai Jiao Tong University, Shanghai, China
4. Shanghai Center for Brain Science and Brain-Inspired Technology, Shanghai, China

**Abstract:** 
The LIDC-IDRI database is commonly used for lung cancer prediction. However, the subjective assessment of nodules by radiologists in this database can lead to label assignment errors and bias during training. To address this, we propose re-labeling the LIDC data using a small dataset of nodules diagnosed by pathological examination. We demonstrate that re-labeling the LIDC nodules using metric learning can improve model performance. We also suggest that building a larger database of nodules with pathological-proven labels is a long-term solution for robust lung cancer prediction.

**Keywords:** 
Pulmonary nodule, Cancer prediction, Metric learning, Re-labeling.

## 1 Introduction  
The LIDC-IDRI database is widely used for lung nodule detection and cancer prediction. However, there are potential issues with the subjective assessment of nodules by radiologists in this database. The assigned malignancy scores may not accurately reflect the true nature of the nodules, leading to label assignment errors and uncertainty. To address this, we introduce the SCH-LND dataset, which contains nodules with pathological-proven labels. We evaluate the performance of the LIDC-driven model using the SCH-LND dataset and propose re-labeling strategies based on interactions with the SCH-LND dataset. We show that re-labeling the LIDC nodules using metric learning improves model performance, especially when new labels are added to the uncertain subset. We also highlight the need for a larger nodule database with pathological-proven labels.

## Materials  
### LIDC-IDRI Database  
We selected nodules identified by at least three radiologists and excluded CT scans with a slice thickness larger than 3 mm. We focused on solid nodules in both the SCH-LND and LIDC databases.  
### Extra Dataset: SCH-LND  
The SCH-LND dataset consists of 180 solid nodules, with 90 benign and 90 malignant nodules. Each nodule in this dataset has been confirmed and diagnosed through immediate pathological examination via biopsy.

## Study Design  
We conducted training and testing in different scenarios and cases to evaluate the performance of the LIDC-driven model in predicting nodule malignancy. We also explored the use of transfer learning to fine-tune the model based on pre-trained LIDC models. We proposed two re-labeling strategies for the LIDC nodules based on interactions with the SCH-LND dataset. The models trained with re-labeled LIDC data created by the metric learning model outperformed the original model, especially when new labels were added to the uncertain subset. Statistical analysis revealed a class imbalance problem in the re-labeled LIDC data, highlighting the need for a larger nodule database with pathological-proven labels.

## Performance Comparisons 


# Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction

## Performance Comparisons

The authors compare the performance of different scenarios and cases in predicting whether nodules are malignant. They use various evaluation metrics such as sensitivity, specificity, precision, accuracy, and F1 score. The results show that there is a bias problem in some scenarios, but others have less bias. They also find that training from scratch on a different dataset does not result in a high-performing model. Therefore, they use transfer learning to fine-tune a pre-trained model.

## Relabeling Strategies

The authors propose two strategies to obtain new ground truth labels for the LIDC database. These strategies aim to correct the bias in the assessment of nodules and utilize uncertain nodules with an average score of 3. The details of these strategies are explained in the following section.

## Methods

The authors present two re-labeling strategies to obtain new ground truth labels for the LIDC database. The first strategy generates the malignancy label using a machine annotator that has been trained on LIDC data and fine-tuned on a different dataset. The second strategy involves ranking the labels of top nodules using a metric-based network that measures the correlation between pairs of nodules. Two modes of re-labeling are proposed in each strategy.

For Mode 1 (Substitute), the re-label outcomes from other label machines are accepted. For Mode 2 (Consensus), the final re-label results are decided by the consensus of label machine outcomes and the original label. This mode leaves behind nodules with the same label and discards controversial ones, which may reduce the amount of data.

The effectiveness of the re-labeling is evaluated using a different dataset to test the model trained with the re-labeled data.

## A Machine Annotator: The State-of-the-Art Nodule Classifier

The machine annotator is a state-of-the-art nodule classifier that has been pre-trained on LIDC data and fine-tuned on a different dataset to predict nodule class. The second strategy involves using a metric learning model to search for the most similar nodules and give new labels. The knowledge from radiologists' assessments is also considered in the re-labeling process.

## Label Induction Using Machine Annotator

The authors use a Siamese Network to train a machine annotator. The network takes pairs of nodules and their labels as inputs and generates feature vectors for each nodule. The goal is to learn representations that can distinguish between nodules of the same class and different classes. During re-labeling, the machine annotator is used to pair each nodule from the training dataset with an under-labeled LIDC nodule and assign a new label based on the similarity scores.

## Experiments and Results

The authors conducted experiments to evaluate the re-labeling strategies. They used different evaluation metrics to measure the performance of each method. The results are presented in tables, showing the sensitivity, specificity, precision, accuracy, and F1 score for each method.

## Implementation

The authors used a 3D ResNet-18 model for their experiments. They trained the models using the Adam optimizer with specific learning rates and conducted 5-fold cross-validation. The experiments were implemented in PyTorch using a single GPU.

## Quantitative Evaluation

The authors evaluated the first re-labeling strategy using the machine annotator. They re-labeled the LIDC nodules using a trained model and tested the performance on a different dataset. The results showed that although the re-labeling fixed the label bias, the model trained with these new labels did not perform well on the test dataset. This indicates that the best model optimized with fine-tuning did not improve the re-labeling performance.

## LIDC re-labeling Strategy

The authors evaluated the performance of different re-labeling methods based on each mode of the re-labeling strategies. They used under-labeled LIDC data with original average scores to compare the methods. The results are presented in a table, showing the sensitivity, specificity, precision, accuracy, and F1 score for each method.

Overall, the authors propose re-labeling strategies to correct bias in the assessment of nodules and improve the performance of pulmonary cancer prediction models. They conduct experiments and provide quantitative evaluations to support their findings. 


Title: Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction

Introduction:
This study addresses the issue of low confidence labels in the LIDC-IDRI database, which is a public database of lung nodules. The authors propose a metric learning-based approach to re-label the LIDC data, aiming to improve performance and increase utilization of the database.

Re-labeling Process:
The authors conducted experiments using two modes: Mode 1 (Substitute) and Mode 2 (Consensus). Mode 2 achieved better overall outcomes but with low specificity. Metric learning, which retrieves similar nodules based on a distance metric, showed better performance compared to general learning. Re-labeling uncertain nodules played a significant role in improving outcomes.

Trade-off between Modes:
There is a trade-off between Mode 1 and Mode 2. Mode 2 tends to retain the bias of the LIDC database, resulting in low specificity and data reduction. However, re-labeling uncertain nodules can help mitigate this bias.

Results:
The authors re-labeled the LIDC database using a Siamese Network trained on another dataset. The re-labeled results aligned well with low malignancy scores. Nodules with an average score of 3 (uncertain data) were mostly re-labeled as benign, leading to improved performance. The re-labeling corrected more than half of the original labels with a score of 4, addressing data bias.

Discussion:
Re-labeling through metric learning differs from general supervised models in two ways: it provides data augmentation and increases label confidence by considering top-ranked similar nodules. This explains why metric learning outperforms general supervised models in re-labeling.

Limitations and Future Work:
The lack of pathological ground truth in the LIDC database makes the re-labeling results uncertain. Clinical information is needed to validate the re-labeling outcomes. The authors emphasize the importance of collecting a large pathological-proven nodule database to address future issues.

Conclusion:
Deep learning models trained on the LIDC database have poor generalization capability due to the absence of clinical information. Re-labeling the LIDC data using a metric learning-based approach improves performance and utilization of the database. Future work involves collecting a large pathological-proven nodule database to enhance accuracy and reliability.

References:
The text includes references to various studies and papers related to lung nodule classification and deep learning techniques. 


This text is a list of references for a research paper titled "Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction." The paper discusses various algorithms and techniques used for the detection and classification of pulmonary nodules in computed tomography (CT) images.

Here are some simplified points about the text:

- The paper references several studies and papers that have contributed to the field of pulmonary nodule detection and classification.
- These studies include the Lung Image Database Consortium (LIDC) data collection process, the LUNA16 challenge, and various deep learning approaches.
- The National Lung Screening Trial (NLST) is also mentioned, which is a large-scale study that investigated the effectiveness of low-dose CT screening for lung cancer.
- The paper discusses the use of transfer learning, multi-scale convolutional neural networks, and ensemble methods for nodule classification.
- Some papers focus on specific aspects of nodule classification, such as malignancy prediction and segmentation.
- The use of sure data and unsure data for training models is also mentioned.

Overall, the text provides a list of relevant references for the research paper on pulmonary nodule detection and classification. 


